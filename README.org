* oai
Emacs package that provide special Org `#+begin_ai` block for communication with LLMs APIs. Text oriented fork of org-ai.
* files
| File           | Purpose                                                     |
|----------------+-------------------------------------------------------------|
| oai.el         | minor Org oai-mode and `C-c C-c` request activation command |
| oai-block.el   | handling ai block                                           |
| oai-restapi.el | connections to REST API (OpenAI-compatible)                 |

* Connection configuration
oai-restapi-con-token - "token-string" or '(:openai "token-string1" :deepseek "token-string2") or nil for reading from "~/.authinfo" or "~/.authinfo.gpg"

oai-restapi-con-service - 'openai or 'deepseek or use parameter at block like this: ```#+begin_ai :service openai```
* Supported services
- OpenAI
- Azure-OpenAI
- Perplexity.ai
- Anthropic
- DeepSeek
- Google
- Github
- Together


oai-restapi-con-service file:org-aire/org-ai-openai.el:164
* Configuration
: git clone https://github.com/Anoncheg1/oai

#+begin_src elisp :results none :exports code :eval no
(add-to-list 'load-path "path/to/oai")
(require 'oai)
(add-hook 'org-mode-hook #'oai-mode) ; oai.el
(setq org-ai-api-creds-token "xxx") ; oai-restapi.el
#+end_src
** How to connect to not supported service? (TODO)
Redefine functions: org-ai--openai-get-token-auth-source, org-ai--get-endpoint, org-ai--get-headers
Set variables:
#+begin_src elisp :results none :exports code :eval no
(setq org-ai-endpoints
      (append org-ai-endpoints
              '((my-new-host "https://api.mynewhost.com/v1/chat/completions"))))
;; Optionally, set it as default
(setq oai-restapi-con-service 'my-new-host)
#+end_src

* Supported “[AI]:” in-text prefixes
- SYS-EVERYWHERE
- SYS - role: system
- AI - role: assistant
- AI_REASON - excluded
- ME - role: user

* Supported parameteres for #+begin_ai
- :model "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free" - string
- :max-tokens 150 - int
- :stream nil - bool/string
- :top-p
- :temperature
- :frequency-penalty
- :presence-penalty
- :service
- :service together - symbol (oai-restapi-con-service)
- :sys-everywhere - string (sys-prompt-for-all-messages) file:org-aire/org-ai.el:131
- :chat :completion (org-ai--request-type) file:org-aire/oai-block.el:93

file:org-aire/org-ai-openai.el:467 (org-ai-stream-completion function)
* debugging
To get full request body and more information set:
: (setopt org-ai-debug-mode t)

Built-in url.el have ability to output request and resonse headers, for that you need to set
: (setq url-debug '(http))
* Extension guide
** How to add post-processing for text
There are a hook ~org-ai-after-chat-insertion-hook~ that accpet two arguments (par1 par2)
- par1: text/role/stop - symbol
- par2: string of chunk


For stream:
#+begin_src text
par1:
role
par2:
assistant

par1:
text
par2:
A

par1:
text
par2:
 question

par1:
end
par2:

#+end_src


Example1:
- file:/home/g/sources/org-ai/org-ai-talk.el::131

Example2 to remove empty lines after AI answer:

#+begin_src elisp :results none :exports code :eval no
(require 'org-ai-expand-block)

(defun my/ai-postprocess (type content)
  (if (equal type 'end)
      (let* ((context (oai-block-p))
             (con-beg (org-element-property :contents-begin context))
             (con-end (org-element-property :contents-end context)))
        (org-ai-remove-distant-empty-lines con-beg con-end))))

(add-hook 'org-ai-after-chat-insertion-hook #'my/ai-postprocess)
#+end_src


Note: text this text located between org-ai--current-insert-position-marker (point-marker) in current buffer.

** org-block functions
- org-in-src-block-p = oai-block-p
* Tests
#+begin_ai :max-tokens 150 :service together :model "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
How to live long?
#+end_ai

#+begin_ai :stream nil :max-tokens 150 :service together :model "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
How to live long?
#+end_ai

#+begin_ai :completion :max-tokens 150 :service together :model "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
How to live long?
#+end_ai

#+begin_ai :stream nil :completion :max-tokens 150 :service together :model "meta-llama/Llama-3.3-70B-Instruct-Turbo-Free"
How to live long?
#+end_ai
